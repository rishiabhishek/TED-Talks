{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from nltk.corpus import stopwords\n",
    "# import re\n",
    "from collections import Counter\n",
    "# import operator\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read TED talk Dataset\n",
    "def read_ted_talk():\n",
    "    file = open(\"content.txt\",\"r\")\n",
    "    return file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_talks = read_ted_talk()[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two reasons companies fail: they only do more of the same, or they only do what's new. To me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation. Both are necessary, but it can be too much of a good thing. Consider Facit. I'm actually old enough to remember them. Facit was a fantastic company. They were born deep in the Swedish forest, and they made the best mechanical calculators in the world. Everybody used them. And what did Facit do when the electronic calculator came along? They continued doing exactly the same. In six months, they went from maximum revenue ... and they were gone. Gone. To me, the irony about the Facit story is hearing about the Facit engineers, who had bought cheap, small electronic calculators in Japan that they used to double-check their calculators. (Laughter) Facit did too much exploitation. But exploration can go wild, too. A few years back, I worked closely alongside a European biotech company. Let's call them OncoSearch. The company was brilliant. They had applications that promised to diagnose, even cure, certain forms of blood cancer. Every day was about creating something new. They were extremely innovative, and the mantra was, \"When we only get it right,\" or even, \"We want it perfect.\" The sad thing is, before they became perfect -- even good enough -- they became obsolete. OncoSearch did too much exploration. I first heard about exploration and exploitation about 15 years ago, when I worked as a visiting scholar at Stanford University. The founder of the idea is Jim March. And to me the power of the idea is its practicality. Exploration. Exploration is about coming up with what's new. It's about search, it's about discovery, it's about new products, it's about new innovations. It's about changing our frontiers. Our heroes are people who have done exploration: Madame Curie, Picasso, Neil Armstrong, Sir Edmund Hillary, etc. I come from Norway; all our heroes are explorers, and they deserve to be. We all know that exploration is risky. We don't know the answers, we don't know if we're going to find them, and we know that the risks are high. Exploitation is the opposite. Exploitation is taking the knowledge we have and making good, better. Exploitation is about making our trains run on time. It's about making good products faster and cheaper. Exploitation is not risky -- in the short term. But if we only exploit, it's very risky in the long term. And I think we all have memories of the famous pop groups who keep singing the same songs again and again, until they become obsolete or even pathetic. That's the risk of exploitation. So if we take a long-term perspective, we explore. If we take a short-term perspective, we exploit. Small children, they explore all day. All day it's about exploration. As we grow older, we explore less because we have more knowledge to exploit on. The same goes for companies. Companies become, by nature, less innovative as they become more competent. And this is, of course, a big worry to CEOs. And I hear very often questions phrased in different ways. For example, \"How can I both effectively run and reinvent my company?\" Or, \"How can I make sure that our company changes before we become obsolete or are hit by a crisis?\" So, doing one well is difficult. Doing both well as the same time is art -- pushing both exploration and exploitation. So one thing we've found is only about two percent of companies are able to effectively explore and exploit at the same time, in parallel. But when they do, the payoffs are huge. So we have lots of great examples. We have Nestlé creating Nespresso, we have Lego going into animated films, Toyota creating the hybrids, Unilever pushing into sustainability -- there are lots of examples, and the benefits are huge. Why is balancing so difficult? I think it's difficult because there are so many traps that keep us where we are. So I'll talk about two, but there are many. So let's talk about the perpetual search trap. We discover something, but we don't have the patience or the persistence to get at it and make it work. So instead of staying with it, we create something new. But the same goes for that, then we're in the vicious circle of actually coming up with ideas but being frustrated. OncoSearch was a good example. A famous example is, of course, Xerox. But we don't only see this in companies. We see this in the public sector as well. We all know that any kind of effective reform of education, research, health care, even defense, takes 10, 15, maybe 20 years to work. But still, we change much more often. We really don't give them the chance. Another trap is the success trap. Facit fell into the success trap. They literally held the future in their hands, but they couldn't see it. They were simply so good at making what they loved doing, that they wouldn't change. We are like that, too. When we know something well, it's difficult to change. Bill Gates has said: \"Success is a lousy teacher. It seduces us into thinking we cannot fail.\" That's the challenge with success. So I think there are some lessons, and I think they apply to us. And they apply to our companies. The first lesson is: get ahead of the crisis. And any company that's able to innovate is actually able to also buy an insurance in the future. Netflix -- they could so easily have been content with earlier generations of distribution, but they always -- and I think they will always -- keep pushing for the next battle. I see other companies that say, \"I'll win the next innovation cycle, whatever it takes.\" Second one: think in multiple time scales. I'll share a chart with you, and I think it's a wonderful one. Any company we look at, taking a one-year perspective and looking at the valuation of the company, innovation typically accounts for only about 30 percent. So when we think one year, innovation isn't really that important. Move ahead, take a 10-year perspective on the same company -- suddenly, innovation and ability to renew account for 70 percent. But companies can't choose. They need to fund the journey and lead the long term. Third: invite talent. I don't think it's possible for any of us to be able to balance exploration and exploitation by ourselves. I think it's a team sport. I think we need to allow challenging. I think the mark of a great company is being open to be challenged, and the mark of a good corporate board is to constructively challenge. I think that's also what good parenting is about. Last one: be skeptical of success. Maybe it's useful to think back at the old triumph marches in Rome, when the generals, after a big victory, were given their celebration. Riding into Rome on the carriage, they always had a companion whispering in their ear, \"Remember, you're only human.\" So I hope I made the point: balancing exploration and exploitation has a huge payoff. But it's difficult, and we need to be conscious. I want to just point out two questions that I think are useful. First question is, looking at your own company: In which areas do you see that the company is at the risk of falling into success traps, of just going on autopilot? And what can you do to challenge? Second question is: When did I explore something new last, and what kind of effect did it have on me? Is that something I should do more of? In my case, yes. So let me leave you with this. Whether you're an explorer by nature or whether you tend to exploit what you already know, don't forget: the beauty is in the balance. Thank you. (Applause)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_talk = raw_talks[0]\n",
    "print(test_talk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_talks(talk_content):\n",
    "    talk_content = talk_content.replace(\"...\",\"\")\n",
    "    talk_content = talk_content.replace(\".\",\" . \")\n",
    "    talk_content = talk_content.replace(\"?\",\" ? \")\n",
    "    talk_content = talk_content.replace(\"!\",\" ! \")\n",
    "    talk_content = talk_content.replace(\";\",\" ; \")\n",
    "    talk_content = talk_content.replace(\",\",\" , \")\n",
    "    talk_content = talk_content.replace(\"?\",\" ? \")\n",
    "    talk_content = talk_content.replace(\":\",\" : \")\n",
    "    talk_content = talk_content.replace(\"(\",\" ( \")\n",
    "    talk_content = talk_content.replace(\")\",\" ) \")\n",
    "    \n",
    "    \n",
    "    talk_content = talk_content.replace(\"\\\"\",\" \\\" \")\n",
    "    talk_content = talk_content.replace(\"--\",\"\")\n",
    "    talk_content = talk_content.replace(\"\\n\",\" \\n \")\n",
    "    return talk_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are two reasons companies fail :  they only do more of the same ,  or they only do what\\'s new .  To me the real ,  real solution to quality growth is figuring out the balance between two activities :  exploration and exploitation .  Both are necessary ,  but it can be too much of a good thing .  Consider Facit .  I\\'m actually old enough to remember them .  Facit was a fantastic company .  They were born deep in the Swedish forest ,  and they made the best mechanical calculators in the world .  Everybody used them .  And what did Facit do when the electronic calculator came along  ?   They continued doing exactly the same .  In six months ,  they went from maximum revenue  and they were gone .  Gone .  To me ,  the irony about the Facit story is hearing about the Facit engineers ,  who had bought cheap ,  small electronic calculators in Japan that they used to double-check their calculators .   ( Laughter )  Facit did too much exploitation .  But exploration can go wild ,  too .  A few years back ,  I worked closely alongside a European biotech company .  Let\\'s call them OncoSearch .  The company was brilliant .  They had applications that promised to diagnose ,  even cure ,  certain forms of blood cancer .  Every day was about creating something new .  They were extremely innovative ,  and the mantra was ,   \" When we only get it right ,  \"  or even ,   \" We want it perfect .  \"  The sad thing is ,  before they became perfect  even good enough  they became obsolete .  OncoSearch did too much exploration .  I first heard about exploration and exploitation about 15 years ago ,  when I worked as a visiting scholar at Stanford University .  The founder of the idea is Jim March .  And to me the power of the idea is its practicality .  Exploration .  Exploration is about coming up with what\\'s new .  It\\'s about search ,  it\\'s about discovery ,  it\\'s about new products ,  it\\'s about new innovations .  It\\'s about changing our frontiers .  Our heroes are people who have done exploration :  Madame Curie ,  Picasso ,  Neil Armstrong ,  Sir Edmund Hillary ,  etc .  I come from Norway ;  all our heroes are explorers ,  and they deserve to be .  We all know that exploration is risky .  We don\\'t know the answers ,  we don\\'t know if we\\'re going to find them ,  and we know that the risks are high .  Exploitation is the opposite .  Exploitation is taking the knowledge we have and making good ,  better .  Exploitation is about making our trains run on time .  It\\'s about making good products faster and cheaper .  Exploitation is not risky  in the short term .  But if we only exploit ,  it\\'s very risky in the long term .  And I think we all have memories of the famous pop groups who keep singing the same songs again and again ,  until they become obsolete or even pathetic .  That\\'s the risk of exploitation .  So if we take a long-term perspective ,  we explore .  If we take a short-term perspective ,  we exploit .  Small children ,  they explore all day .  All day it\\'s about exploration .  As we grow older ,  we explore less because we have more knowledge to exploit on .  The same goes for companies .  Companies become ,  by nature ,  less innovative as they become more competent .  And this is ,  of course ,  a big worry to CEOs .  And I hear very often questions phrased in different ways .  For example ,   \" How can I both effectively run and reinvent my company  ?   \"  Or ,   \" How can I make sure that our company changes before we become obsolete or are hit by a crisis  ?   \"  So ,  doing one well is difficult .  Doing both well as the same time is art  pushing both exploration and exploitation .  So one thing we\\'ve found is only about two percent of companies are able to effectively explore and exploit at the same time ,  in parallel .  But when they do ,  the payoffs are huge .  So we have lots of great examples .  We have Nestlé creating Nespresso ,  we have Lego going into animated films ,  Toyota creating the hybrids ,  Unilever pushing into sustainability  there are lots of examples ,  and the benefits are huge .  Why is balancing so difficult  ?   I think it\\'s difficult because there are so many traps that keep us where we are .  So I\\'ll talk about two ,  but there are many .  So let\\'s talk about the perpetual search trap .  We discover something ,  but we don\\'t have the patience or the persistence to get at it and make it work .  So instead of staying with it ,  we create something new .  But the same goes for that ,  then we\\'re in the vicious circle of actually coming up with ideas but being frustrated .  OncoSearch was a good example .  A famous example is ,  of course ,  Xerox .  But we don\\'t only see this in companies .  We see this in the public sector as well .  We all know that any kind of effective reform of education ,  research ,  health care ,  even defense ,  takes 10 ,  15 ,  maybe 20 years to work .  But still ,  we change much more often .  We really don\\'t give them the chance .  Another trap is the success trap .  Facit fell into the success trap .  They literally held the future in their hands ,  but they couldn\\'t see it .  They were simply so good at making what they loved doing ,  that they wouldn\\'t change .  We are like that ,  too .  When we know something well ,  it\\'s difficult to change .  Bill Gates has said :   \" Success is a lousy teacher .  It seduces us into thinking we cannot fail .  \"  That\\'s the challenge with success .  So I think there are some lessons ,  and I think they apply to us .  And they apply to our companies .  The first lesson is :  get ahead of the crisis .  And any company that\\'s able to innovate is actually able to also buy an insurance in the future .  Netflix  they could so easily have been content with earlier generations of distribution ,  but they always  and I think they will always  keep pushing for the next battle .  I see other companies that say ,   \" I\\'ll win the next innovation cycle ,  whatever it takes .  \"  Second one :  think in multiple time scales .  I\\'ll share a chart with you ,  and I think it\\'s a wonderful one .  Any company we look at ,  taking a one-year perspective and looking at the valuation of the company ,  innovation typically accounts for only about 30 percent .  So when we think one year ,  innovation isn\\'t really that important .  Move ahead ,  take a 10-year perspective on the same company  suddenly ,  innovation and ability to renew account for 70 percent .  But companies can\\'t choose .  They need to fund the journey and lead the long term .  Third :  invite talent .  I don\\'t think it\\'s possible for any of us to be able to balance exploration and exploitation by ourselves .  I think it\\'s a team sport .  I think we need to allow challenging .  I think the mark of a great company is being open to be challenged ,  and the mark of a good corporate board is to constructively challenge .  I think that\\'s also what good parenting is about .  Last one :  be skeptical of success .  Maybe it\\'s useful to think back at the old triumph marches in Rome ,  when the generals ,  after a big victory ,  were given their celebration .  Riding into Rome on the carriage ,  they always had a companion whispering in their ear ,   \" Remember ,  you\\'re only human .  \"  So I hope I made the point :  balancing exploration and exploitation has a huge payoff .  But it\\'s difficult ,  and we need to be conscious .  I want to just point out two questions that I think are useful .  First question is ,  looking at your own company :  In which areas do you see that the company is at the risk of falling into success traps ,  of just going on autopilot  ?   And what can you do to challenge  ?   Second question is :  When did I explore something new last ,  and what kind of effect did it have on me  ?   Is that something I should do more of  ?   In my case ,  yes .  So let me leave you with this .  Whether you\\'re an explorer by nature or whether you tend to exploit what you already know ,  don\\'t forget :  the beauty is in the balance .  Thank you .   ( Applause )  \\n '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_talks(raw_talks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_talks = [preprocess_talks(talk) for talk in raw_talks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_talks(talks):\n",
    "    tokenized_talks = []\n",
    "    for talk in talks:\n",
    "        tokenized_talks.append(talk.lower().split())\n",
    "    return tokenized_talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_talks = tokenize_talks(processed_talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vocab(talks, keep_count = 10):\n",
    "    '''\n",
    "    Param : talks, keep_count = 10\n",
    "    Return : vocab, vocab_to_int, int_to_vocab\n",
    "    '''\n",
    "    vocab = Counter()\n",
    "    for talk in talks:\n",
    "        vocab += Counter(talk)\n",
    "    \n",
    "    #Remove tokens from vocab having less than 10 counts\n",
    "    vocab = [token for token,count in vocab.items() if count > keep_count]\n",
    "    int_to_vocab = {i:word for i,word in enumerate(vocab)}\n",
    "    vocab_to_int = {word:i for i,word in int_to_vocab.items()}\n",
    "    \n",
    "    return vocab, vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab,vocab_to_int,int_to_vocab = create_vocab(tokenized_talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'pretty',\n",
       " 1: 'feel',\n",
       " 2: 'fuel',\n",
       " 3: 'barely',\n",
       " 4: 'fgm',\n",
       " 5: 'sports',\n",
       " 6: 'advocates',\n",
       " 7: 'tragically',\n",
       " 8: 'minimize',\n",
       " 9: 'certainty',\n",
       " 10: 'stocks',\n",
       " 11: 'shrinking',\n",
       " 12: 'never',\n",
       " 13: 'rounded',\n",
       " 14: 'correlated',\n",
       " 15: 'blind',\n",
       " 16: 'golden',\n",
       " 17: 'straw',\n",
       " 18: 'whereas',\n",
       " 19: 'exquisitely',\n",
       " 20: 'damaged',\n",
       " 21: 'tiger',\n",
       " 22: \"you've\",\n",
       " 23: 'stain',\n",
       " 24: 'grade',\n",
       " 25: 'cafe',\n",
       " 26: 'waving',\n",
       " 27: 'precursor',\n",
       " 28: 'lid',\n",
       " 29: 'electromagnetic',\n",
       " 30: 'joshua',\n",
       " 31: 'regulate',\n",
       " 32: 'markers',\n",
       " 33: 'approval',\n",
       " 34: 'software',\n",
       " 35: 'fronts',\n",
       " 36: \"tom's\",\n",
       " 37: 'prozac',\n",
       " 38: 'harmony',\n",
       " 39: 'mud',\n",
       " 40: 'coastal',\n",
       " 41: 'exploitation',\n",
       " 42: 'tickets',\n",
       " 43: 'happens',\n",
       " 44: 'blinded',\n",
       " 45: 'gothic',\n",
       " 46: 'washes',\n",
       " 47: 'prize',\n",
       " 48: 'survives',\n",
       " 49: 'stands',\n",
       " 50: 'hurricane',\n",
       " 51: 'forming',\n",
       " 52: 'honey',\n",
       " 53: 'recycling',\n",
       " 54: 'stretches',\n",
       " 55: 'butt',\n",
       " 56: 'disaster',\n",
       " 57: 'distort',\n",
       " 58: 'performing',\n",
       " 59: 'existence',\n",
       " 60: 'davis',\n",
       " 61: 'accused',\n",
       " 62: 'spike',\n",
       " 63: 'laboratory',\n",
       " 64: 'two-dimensional',\n",
       " 65: 'fuss',\n",
       " 66: 'medicines',\n",
       " 67: 'monster',\n",
       " 68: 'stimulate',\n",
       " 69: 'whisper',\n",
       " 70: 'clicks',\n",
       " 71: 'celebrated',\n",
       " 72: 'eliminate',\n",
       " 73: 'asl',\n",
       " 74: 'systems',\n",
       " 75: 'independent',\n",
       " 76: 'survival',\n",
       " 77: 'headquarters',\n",
       " 78: 'relieve',\n",
       " 79: 'mayor',\n",
       " 80: 'streets',\n",
       " 81: 'colombia',\n",
       " 82: 'rebel',\n",
       " 83: 'spacecraft',\n",
       " 84: 'mistrust',\n",
       " 85: 'deregulation',\n",
       " 86: 'prayer',\n",
       " 87: 'litigation',\n",
       " 88: 'summary',\n",
       " 89: 'answered',\n",
       " 90: 'probiotics',\n",
       " 91: 'practices',\n",
       " 92: 'illusions',\n",
       " 93: 'reaching',\n",
       " 94: 'projected',\n",
       " 95: 'volunteer',\n",
       " 96: 'make',\n",
       " 97: 'provoked',\n",
       " 98: 'outlets',\n",
       " 99: 'victims',\n",
       " 100: 'bomber',\n",
       " 101: 'particle',\n",
       " 102: 'scan',\n",
       " 103: 'buttons',\n",
       " 104: 'fascinating',\n",
       " 105: 'copernicus',\n",
       " 106: 'glands',\n",
       " 107: 'phrases',\n",
       " 108: 'revisit',\n",
       " 109: 'plants',\n",
       " 110: 'assured',\n",
       " 111: 'cooperate',\n",
       " 112: 'recruitment',\n",
       " 113: 'slaves',\n",
       " 114: 'centuries',\n",
       " 115: 'abundance',\n",
       " 116: 'fiddle',\n",
       " 117: 'limb',\n",
       " 118: 'leaps',\n",
       " 119: 'rocks',\n",
       " 120: 'collecting',\n",
       " 121: 'eye',\n",
       " 122: 'address',\n",
       " 123: 'wal-mart',\n",
       " 124: 'empathy',\n",
       " 125: 'slow',\n",
       " 126: 'sage',\n",
       " 127: 'animation',\n",
       " 128: 'noun',\n",
       " 129: 'notation',\n",
       " 130: 'potent',\n",
       " 131: 'sex',\n",
       " 132: 'weeks',\n",
       " 133: 'resume',\n",
       " 134: 'decreased',\n",
       " 135: 'versus',\n",
       " 136: 'sketches',\n",
       " 137: '1984',\n",
       " 138: 'bathrooms',\n",
       " 139: 'insulin',\n",
       " 140: 'day',\n",
       " 141: 'all',\n",
       " 142: 'shaking',\n",
       " 143: 'hungarian',\n",
       " 144: 'careers',\n",
       " 145: 'adventure',\n",
       " 146: 'merging',\n",
       " 147: 'considerable',\n",
       " 148: 'aspect',\n",
       " 149: 'low-level',\n",
       " 150: 'theories',\n",
       " 151: 'wish',\n",
       " 152: 'artwork',\n",
       " 153: 'turing',\n",
       " 154: 'varied',\n",
       " 155: 'ridicule',\n",
       " 156: 'charlie',\n",
       " 157: 'reminded',\n",
       " 158: 'humankind',\n",
       " 159: 'anesthesia',\n",
       " 160: 'cardiovascular',\n",
       " 161: 'orbits',\n",
       " 162: 'sunny',\n",
       " 163: 'collaboration',\n",
       " 164: 'functionality',\n",
       " 165: \"years'\",\n",
       " 166: 'healthcare',\n",
       " 167: 'kept',\n",
       " 168: 'superior',\n",
       " 169: 'dubai',\n",
       " 170: 'fall',\n",
       " 171: 'economical',\n",
       " 172: 'norms',\n",
       " 173: 'aesthetic',\n",
       " 174: 'guesses',\n",
       " 175: 'parody',\n",
       " 176: 'survivors',\n",
       " 177: 'disruptive',\n",
       " 178: 'enrich',\n",
       " 179: 'amira',\n",
       " 180: 'disclose',\n",
       " 181: 'blink',\n",
       " 182: 'forum',\n",
       " 183: 'counterparts',\n",
       " 184: 'yr',\n",
       " 185: 'scouts',\n",
       " 186: 'frugal',\n",
       " 187: 'rampant',\n",
       " 188: 'sunshine',\n",
       " 189: 'speculative',\n",
       " 190: 'own',\n",
       " 191: 'buffett',\n",
       " 192: 'drawing',\n",
       " 193: 'believe',\n",
       " 194: 'joan',\n",
       " 195: 'fulfilled',\n",
       " 196: 'else',\n",
       " 197: 'marshall',\n",
       " 198: 'solar-powered',\n",
       " 199: 'improve',\n",
       " 200: 'candle',\n",
       " 201: 'digitize',\n",
       " 202: 'ls',\n",
       " 203: 'linear',\n",
       " 204: 'sake',\n",
       " 205: 'helps',\n",
       " 206: 'ammunition',\n",
       " 207: 'acquisition',\n",
       " 208: 'historic',\n",
       " 209: 'pancreatic',\n",
       " 210: 'boils',\n",
       " 211: 'extensive',\n",
       " 212: \"name's\",\n",
       " 213: 'suspected',\n",
       " 214: 'guy',\n",
       " 215: 'steal',\n",
       " 216: 'tissues',\n",
       " 217: 'link',\n",
       " 218: 'recovering',\n",
       " 219: 'hurts',\n",
       " 220: 'removing',\n",
       " 221: 'babbage',\n",
       " 222: 'manipulating',\n",
       " 223: 'progressed',\n",
       " 224: 'lifestyle',\n",
       " 225: 'transformations',\n",
       " 226: 'poet',\n",
       " 227: 'soul',\n",
       " 228: 'latest',\n",
       " 229: 'johns',\n",
       " 230: 'abusive',\n",
       " 231: 'milliseconds',\n",
       " 232: 'concentrated',\n",
       " 233: 'toys',\n",
       " 234: 'hiring',\n",
       " 235: 'parking',\n",
       " 236: 'walter',\n",
       " 237: 'low-cost',\n",
       " 238: 'banks',\n",
       " 239: 'grasping',\n",
       " 240: 'exceptions',\n",
       " 241: 'pulls',\n",
       " 242: 'ignore',\n",
       " 243: 'royal',\n",
       " 244: 'assault',\n",
       " 245: 'mozart',\n",
       " 246: 'len',\n",
       " 247: 'fine',\n",
       " 248: 'thinner',\n",
       " 249: 'off',\n",
       " 250: 'internet',\n",
       " 251: 'wastewater',\n",
       " 252: 'adaptability',\n",
       " 253: 'fry',\n",
       " 254: 'prototyping',\n",
       " 255: 'varieties',\n",
       " 256: 'thai',\n",
       " 257: 'virgin',\n",
       " 258: 'lolcats',\n",
       " 259: 'kong',\n",
       " 260: 'post-traumatic',\n",
       " 261: 'leverage',\n",
       " 262: 'basking',\n",
       " 263: 'sparked',\n",
       " 264: 'gets',\n",
       " 265: 'central',\n",
       " 266: 'stimulation',\n",
       " 267: 'incremental',\n",
       " 268: 'shocking',\n",
       " 269: 'attacking',\n",
       " 270: 'pet',\n",
       " 271: 'prefer',\n",
       " 272: 'ma',\n",
       " 273: 'target',\n",
       " 274: 'terminator',\n",
       " 275: 'guarantee',\n",
       " 276: \"'s\",\n",
       " 277: 'flick',\n",
       " 278: 'reducing',\n",
       " 279: 'gasses',\n",
       " 280: 'wiped',\n",
       " 281: 'piles',\n",
       " 282: 'pedestrian',\n",
       " 283: 'frontier',\n",
       " 284: 'competing',\n",
       " 285: 'laughing',\n",
       " 286: 'clubs',\n",
       " 287: 'leading',\n",
       " 288: 'tutoring',\n",
       " 289: 'mating',\n",
       " 290: 'browsing',\n",
       " 291: 'explained',\n",
       " 292: 'heritage',\n",
       " 293: 'ultraviolet',\n",
       " 294: 'spaghetti',\n",
       " 295: 'clear',\n",
       " 296: \"man's\",\n",
       " 297: 'bottleneck',\n",
       " 298: 'campaigning',\n",
       " 299: 'astronomers',\n",
       " 300: 'top',\n",
       " 301: 'tortured',\n",
       " 302: 'transcripts',\n",
       " 303: 'medicare',\n",
       " 304: 'porridge',\n",
       " 305: \"we're\",\n",
       " 306: 'interrogation',\n",
       " 307: 'tedsters',\n",
       " 308: 'signs',\n",
       " 309: 'cures',\n",
       " 310: 'somalia',\n",
       " 311: 'military',\n",
       " 312: 'attacks',\n",
       " 313: \"others'\",\n",
       " 314: 'fragment',\n",
       " 315: 'helped',\n",
       " 316: 'nick',\n",
       " 317: 'dared',\n",
       " 318: 'homosexual',\n",
       " 319: 'aimed',\n",
       " 320: 'highlights',\n",
       " 321: 'millennium',\n",
       " 322: 'thrown',\n",
       " 323: 'other',\n",
       " 324: 'agriculture',\n",
       " 325: 'dictators',\n",
       " 326: 'voiceless',\n",
       " 327: '1965',\n",
       " 328: 'ventilated',\n",
       " 329: 'slash',\n",
       " 330: 'couch',\n",
       " 331: 'severity',\n",
       " 332: 'suspension',\n",
       " 333: 'held',\n",
       " 334: 'artifact',\n",
       " 335: 'craig',\n",
       " 336: 'protected',\n",
       " 337: 'retail',\n",
       " 338: 'encrypted',\n",
       " 339: 'curiously',\n",
       " 340: 'gangs',\n",
       " 341: 'among',\n",
       " 342: 'star',\n",
       " 343: 'compact',\n",
       " 344: 'coats',\n",
       " 345: 'tape',\n",
       " 346: 'quarters',\n",
       " 347: 'ahmed',\n",
       " 348: 'boreal',\n",
       " 349: 'image',\n",
       " 350: 'fate',\n",
       " 351: 'cairo',\n",
       " 352: 'grabs',\n",
       " 353: 'branching',\n",
       " 354: 'depression',\n",
       " 355: 'tough',\n",
       " 356: 'programming',\n",
       " 357: 'plumbing',\n",
       " 358: 'symmetries',\n",
       " 359: 'trust',\n",
       " 360: 'explorers',\n",
       " 361: 'helix',\n",
       " 362: 'traces',\n",
       " 363: 'curved',\n",
       " 364: 'beloved',\n",
       " 365: 'long',\n",
       " 366: 'bloomberg',\n",
       " 367: 'gum',\n",
       " 368: 'un',\n",
       " 369: 'flowers',\n",
       " 370: 'number',\n",
       " 371: 'surge',\n",
       " 372: 'participant',\n",
       " 373: 'assumed',\n",
       " 374: 'regulated',\n",
       " 375: 'scoop',\n",
       " 376: 'mobilized',\n",
       " 377: 'substances',\n",
       " 378: 'millimeter',\n",
       " 379: 'appointment',\n",
       " 380: 'import',\n",
       " 381: 'shaming',\n",
       " 382: 'climb',\n",
       " 383: 'shifts',\n",
       " 384: 'convicted',\n",
       " 385: 'ward',\n",
       " 386: 'livelihood',\n",
       " 387: 'i’m',\n",
       " 388: 'consultation',\n",
       " 389: 'with',\n",
       " 390: 'ninja',\n",
       " 391: 'manuscripts',\n",
       " 392: 'phase',\n",
       " 393: 'gathered',\n",
       " 394: 'wright',\n",
       " 395: 'lemon',\n",
       " 396: 'application',\n",
       " 397: 'brass',\n",
       " 398: 'mutilated',\n",
       " 399: 'denied',\n",
       " 400: 'dichotomy',\n",
       " 401: 'controls',\n",
       " 402: 'governors',\n",
       " 403: 'abortion',\n",
       " 404: 'schooling',\n",
       " 405: 'unlike',\n",
       " 406: 'efficiently',\n",
       " 407: 'orthodox',\n",
       " 408: 'reflect',\n",
       " 409: 'negativity',\n",
       " 410: 'ridiculous',\n",
       " 411: 'leaves',\n",
       " 412: 'ball',\n",
       " 413: 'vest',\n",
       " 414: 'sliver',\n",
       " 415: 'is',\n",
       " 416: 'palestine',\n",
       " 417: 'estimates',\n",
       " 418: 'complexity',\n",
       " 419: 'inquiry',\n",
       " 420: 'scottish',\n",
       " 421: 'cells',\n",
       " 422: 'insidious',\n",
       " 423: 'são',\n",
       " 424: 'looks',\n",
       " 425: 'sm',\n",
       " 426: '1950',\n",
       " 427: 'multicultural',\n",
       " 428: 'every',\n",
       " 429: 'hi',\n",
       " 430: 'there',\n",
       " 431: 'bites',\n",
       " 432: 'neighboring',\n",
       " 433: 'achieve',\n",
       " 434: 'agreed',\n",
       " 435: '18-year-old',\n",
       " 436: 'wondering',\n",
       " 437: 'pretending',\n",
       " 438: 'penny',\n",
       " 439: 'nicely',\n",
       " 440: 'biomaterials',\n",
       " 441: 'lacking',\n",
       " 442: 'nerdy',\n",
       " 443: 'seldom',\n",
       " 444: 'simple',\n",
       " 445: 'vegetables',\n",
       " 446: 'colors',\n",
       " 447: 'simplified',\n",
       " 448: 'syringe',\n",
       " 449: 'attempted',\n",
       " 450: 'microscope',\n",
       " 451: 'distinctive',\n",
       " 452: 'continent',\n",
       " 453: '4',\n",
       " 454: 'storage',\n",
       " 455: 'superheroes',\n",
       " 456: 'dancing',\n",
       " 457: 'risen',\n",
       " 458: 'matches',\n",
       " 459: 'devised',\n",
       " 460: 'automobiles',\n",
       " 461: 'energy-efficient',\n",
       " 462: 'bed',\n",
       " 463: 'hardworking',\n",
       " 464: 'strips',\n",
       " 465: 'terrestrial',\n",
       " 466: 'bloom',\n",
       " 467: 'hence',\n",
       " 468: 'recovery',\n",
       " 469: 'hosted',\n",
       " 470: 'depended',\n",
       " 471: 'lastly',\n",
       " 472: 'defenders',\n",
       " 473: 'missiles',\n",
       " 474: 'countryside',\n",
       " 475: '200',\n",
       " 476: 'magazines',\n",
       " 477: 'outsider',\n",
       " 478: 'requires',\n",
       " 479: 'cases',\n",
       " 480: 'manufactured',\n",
       " 481: 'non-violent',\n",
       " 482: 'grocery',\n",
       " 483: 'argue',\n",
       " 484: 'belong',\n",
       " 485: 'compensation',\n",
       " 486: 'wales',\n",
       " 487: 'paradox',\n",
       " 488: 'committing',\n",
       " 489: 'sane',\n",
       " 490: 'capsule',\n",
       " 491: 'nurse',\n",
       " 492: 'cultures',\n",
       " 493: 'mammography',\n",
       " 494: 'vocal',\n",
       " 495: 'simplify',\n",
       " 496: 'hay',\n",
       " 497: 'mac',\n",
       " 498: 'amazing',\n",
       " 499: 'itself',\n",
       " 500: 'pleasant',\n",
       " 501: 'cafeteria',\n",
       " 502: 'accelerate',\n",
       " 503: 'wedding',\n",
       " 504: 'neither',\n",
       " 505: 'bags',\n",
       " 506: 'arrival',\n",
       " 507: 'lie',\n",
       " 508: 'slight',\n",
       " 509: 'ideally',\n",
       " 510: 'navigating',\n",
       " 511: 'triggered',\n",
       " 512: 'destination',\n",
       " 513: 'romance',\n",
       " 514: 'dr',\n",
       " 515: 'melting',\n",
       " 516: 'autonomously',\n",
       " 517: 'marker',\n",
       " 518: 'approached',\n",
       " 519: 'well-known',\n",
       " 520: 'confronted',\n",
       " 521: '15',\n",
       " 522: 'charm',\n",
       " 523: 'spontaneously',\n",
       " 524: 'compensate',\n",
       " 525: 'beliefs',\n",
       " 526: 'scaled',\n",
       " 527: 'bells',\n",
       " 528: 'types',\n",
       " 529: 'corner',\n",
       " 530: 'attempt',\n",
       " 531: 'physicists',\n",
       " 532: 'acts',\n",
       " 533: 'applaud',\n",
       " 534: 'popped',\n",
       " 535: \"anybody's\",\n",
       " 536: 'fertilizers',\n",
       " 537: 'rockefeller',\n",
       " 538: 'broccoli',\n",
       " 539: \"darwin's\",\n",
       " 540: 'tomorrow',\n",
       " 541: 'logged',\n",
       " 542: 'highlighted',\n",
       " 543: 'afternoon',\n",
       " 544: 'yet',\n",
       " 545: 'fatigue',\n",
       " 546: 'assignment',\n",
       " 547: 'mechanics',\n",
       " 548: 'circumstance',\n",
       " 549: 'makeshift',\n",
       " 550: 'implicit',\n",
       " 551: 'continually',\n",
       " 552: 'invent',\n",
       " 553: 'primarily',\n",
       " 554: 'brown',\n",
       " 555: 'collect',\n",
       " 556: 'grape',\n",
       " 557: 'enjoy',\n",
       " 558: 'blogosphere',\n",
       " 559: 'dp',\n",
       " 560: 'crackers',\n",
       " 561: 'mobile',\n",
       " 562: 'restaurant',\n",
       " 563: 'attachment',\n",
       " 564: 'crude',\n",
       " 565: 'barefoot',\n",
       " 566: 'e8',\n",
       " 567: 'researched',\n",
       " 568: 'canvas',\n",
       " 569: 'disproportionate',\n",
       " 570: 'enforced',\n",
       " 571: 'compounds',\n",
       " 572: 'workers',\n",
       " 573: 'horses',\n",
       " 574: 'sticking',\n",
       " 575: 'studied',\n",
       " 576: 'tight',\n",
       " 577: 'reveal',\n",
       " 578: 'secrets',\n",
       " 579: 'millimeters',\n",
       " 580: 'criticize',\n",
       " 581: 'something',\n",
       " 582: 'tool',\n",
       " 583: 'unlimited',\n",
       " 584: 'disrupted',\n",
       " 585: 'alien',\n",
       " 586: 'worthy',\n",
       " 587: 'pyramids',\n",
       " 588: 'relational',\n",
       " 589: '2011',\n",
       " 590: 'recreation',\n",
       " 591: 'freaking',\n",
       " 592: 'fruit',\n",
       " 593: 'october',\n",
       " 594: 'less',\n",
       " 595: 'buck',\n",
       " 596: 'successes',\n",
       " 597: 'onstage',\n",
       " 598: 'dexterity',\n",
       " 599: 'great',\n",
       " 600: 'neatly',\n",
       " 601: 'options',\n",
       " 602: 'adorable',\n",
       " 603: 'february',\n",
       " 604: 'standards',\n",
       " 605: 'dare',\n",
       " 606: 'facade',\n",
       " 607: 'solution',\n",
       " 608: 'invention',\n",
       " 609: 'bay',\n",
       " 610: 'counting',\n",
       " 611: 'neighbor',\n",
       " 612: 'somehow',\n",
       " 613: 'judy',\n",
       " 614: 'complaint',\n",
       " 615: 'addict',\n",
       " 616: 'coupled',\n",
       " 617: 'salute',\n",
       " 618: 'widespread',\n",
       " 619: '17th',\n",
       " 620: '23',\n",
       " 621: 'contained',\n",
       " 622: 'lay',\n",
       " 623: 'came',\n",
       " 624: 'font',\n",
       " 625: 'r',\n",
       " 626: 'intensely',\n",
       " 627: 'newspapers',\n",
       " 628: 'definitions',\n",
       " 629: 'evils',\n",
       " 630: 'violin',\n",
       " 631: 'humor',\n",
       " 632: 'siege',\n",
       " 633: 'breathe',\n",
       " 634: 'separation',\n",
       " 635: 'life',\n",
       " 636: 'registered',\n",
       " 637: 'portuguese',\n",
       " 638: 'uniqueness',\n",
       " 639: 'malaria',\n",
       " 640: 'coin',\n",
       " 641: 'nutshell',\n",
       " 642: 'shine',\n",
       " 643: 'boil',\n",
       " 644: 'ethiopian',\n",
       " 645: 'monitored',\n",
       " 646: 'wishing',\n",
       " 647: 'dolly',\n",
       " 648: 'occurred',\n",
       " 649: 'brave',\n",
       " 650: 'stepped',\n",
       " 651: 'dragged',\n",
       " 652: 'autos',\n",
       " 653: 'cleared',\n",
       " 654: 'denial',\n",
       " 655: 'belgium',\n",
       " 656: 'count',\n",
       " 657: 'doors',\n",
       " 658: 'labyrinth',\n",
       " 659: 'crashed',\n",
       " 660: 'gulf',\n",
       " 661: 'war',\n",
       " 662: 'innocence',\n",
       " 663: 'ben',\n",
       " 664: 'wasps',\n",
       " 665: 'bind',\n",
       " 666: '6',\n",
       " 667: 'pottery',\n",
       " 668: 'uh-oh',\n",
       " 669: 'rig',\n",
       " 670: 'sticky',\n",
       " 671: 'vacant',\n",
       " 672: 'subjected',\n",
       " 673: 'hip',\n",
       " 674: 'film',\n",
       " 675: 'dissonance',\n",
       " 676: 'designs',\n",
       " 677: 'sparks',\n",
       " 678: 'intervention',\n",
       " 679: 'intrinsic',\n",
       " 680: 'academy',\n",
       " 681: 'orbital',\n",
       " 682: 'flute',\n",
       " 683: 'dentists',\n",
       " 684: 'tar',\n",
       " 685: 'justify',\n",
       " 686: 'bible',\n",
       " 687: 'underway',\n",
       " 688: 'reserves',\n",
       " 689: 'defining',\n",
       " 690: 'anyhow',\n",
       " 691: 'analytical',\n",
       " 692: 'explain',\n",
       " 693: 'representative',\n",
       " 694: 'pits',\n",
       " 695: 'hormonal',\n",
       " 696: 'shared',\n",
       " 697: 'bust',\n",
       " 698: 'disastrous',\n",
       " 699: 'spill',\n",
       " 700: 'bg',\n",
       " 701: 'arrow',\n",
       " 702: 'mortal',\n",
       " 703: 'contagion',\n",
       " 704: 'feed',\n",
       " 705: 'allows',\n",
       " 706: 'nickname',\n",
       " 707: 'iq',\n",
       " 708: 'untold',\n",
       " 709: 'kurzweil',\n",
       " 710: 'timely',\n",
       " 711: 'batman',\n",
       " 712: 'rains',\n",
       " 713: 'nairobi',\n",
       " 714: 'intervening',\n",
       " 715: 'zimbabwe',\n",
       " 716: 'friendship',\n",
       " 717: 'illuminate',\n",
       " 718: 'wages',\n",
       " 719: 'bitter',\n",
       " 720: 'moses',\n",
       " 721: 'coaches',\n",
       " 722: 'spark',\n",
       " 723: 'cutting',\n",
       " 724: 'unicorn',\n",
       " 725: 'logo',\n",
       " 726: 'colossal',\n",
       " 727: \"technology's\",\n",
       " 728: 'fishermen',\n",
       " 729: 'affairs',\n",
       " 730: 'logic',\n",
       " 731: 'resolved',\n",
       " 732: 'really',\n",
       " 733: 'eternal',\n",
       " 734: 'delivery',\n",
       " 735: 'economy',\n",
       " 736: 'mistakenly',\n",
       " 737: 'carbs',\n",
       " 738: 'onset',\n",
       " 739: 'expenses',\n",
       " 740: 'uploaded',\n",
       " 741: 'inventive',\n",
       " 742: 'clients',\n",
       " 743: 'an',\n",
       " 744: 'synchrony',\n",
       " 745: 'caused',\n",
       " 746: 'sharp',\n",
       " 747: 'commonly',\n",
       " 748: 'adventurous',\n",
       " 749: 'greatest',\n",
       " 750: 'polar',\n",
       " 751: 'visually',\n",
       " 752: 'gotten',\n",
       " 753: 'latin',\n",
       " 754: 'outer',\n",
       " 755: 'workforce',\n",
       " 756: 'storycorps',\n",
       " 757: 'clerk',\n",
       " 758: 'employment',\n",
       " 759: 'him',\n",
       " 760: 'pheromones',\n",
       " 761: 'emitting',\n",
       " 762: 'woods',\n",
       " 763: 'el',\n",
       " 764: 'pinnacle',\n",
       " 765: 'road',\n",
       " 766: 'alike',\n",
       " 767: 'supply',\n",
       " 768: 'admiration',\n",
       " 769: 'genetics',\n",
       " 770: 'cheaper',\n",
       " 771: 'dickinson',\n",
       " 772: 'realizes',\n",
       " 773: 'broke',\n",
       " 774: 'falling',\n",
       " 775: 'pardon',\n",
       " 776: 'couple',\n",
       " 777: 'unnecessary',\n",
       " 778: 'dissent',\n",
       " 779: 'evolve',\n",
       " 780: 'occupation',\n",
       " 781: 'resemblance',\n",
       " 782: 'diffusion',\n",
       " 783: 'tedglobal',\n",
       " 784: 'income',\n",
       " 785: 'encryption',\n",
       " 786: 'propulsion',\n",
       " 787: 'households',\n",
       " 788: 'wisely',\n",
       " 789: 'inside',\n",
       " 790: '750',\n",
       " 791: 'satellites',\n",
       " 792: 'past',\n",
       " 793: 'oily',\n",
       " 794: 'afford',\n",
       " 795: 'gratefulness',\n",
       " 796: 'panama',\n",
       " 797: 'investing',\n",
       " 798: 'puppets',\n",
       " 799: 'ph',\n",
       " 800: 'take-home',\n",
       " 801: 'suicide',\n",
       " 802: 'quantities',\n",
       " 803: 'peer-reviewed',\n",
       " 804: 'marathon',\n",
       " 805: 'factories',\n",
       " 806: 'acknowledged',\n",
       " 807: 'profitable',\n",
       " 808: 'stays',\n",
       " 809: 'updated',\n",
       " 810: 'imprisonment',\n",
       " 811: 'demonstration',\n",
       " 812: 'inconvenient',\n",
       " 813: 'ports',\n",
       " 814: 'public',\n",
       " 815: 'code',\n",
       " 816: 'parks',\n",
       " 817: 'gracefully',\n",
       " 818: 'cockroaches',\n",
       " 819: 'tapping',\n",
       " 820: 'extraordinarily',\n",
       " 821: 'breton',\n",
       " 822: 'very',\n",
       " 823: 'partial',\n",
       " 824: 'carbon',\n",
       " 825: 'sighted',\n",
       " 826: 'instinct',\n",
       " 827: 'enjoyed',\n",
       " 828: 'chapters',\n",
       " 829: 'poses',\n",
       " 830: 'neurogenesis',\n",
       " 831: 'contributed',\n",
       " 832: 'electrons',\n",
       " 833: 'self',\n",
       " 834: 'divorce',\n",
       " 835: 'explanations',\n",
       " 836: 'jerry',\n",
       " 837: 'outwards',\n",
       " 838: 'we’re',\n",
       " 839: 'says',\n",
       " 840: 'supermarket',\n",
       " 841: 'groundbreaking',\n",
       " 842: 'eradication',\n",
       " 843: 'simulator',\n",
       " 844: 'liver',\n",
       " 845: 'japanese',\n",
       " 846: 'fabulous',\n",
       " 847: 'shuttle',\n",
       " 848: 'invite',\n",
       " 849: 'lighter',\n",
       " 850: '$1',\n",
       " 851: 'noted',\n",
       " 852: 'born',\n",
       " 853: 'fails',\n",
       " 854: 'scientist',\n",
       " 855: 'variation',\n",
       " 856: 'interacted',\n",
       " 857: 'always',\n",
       " 858: 'smoking',\n",
       " 859: 'hype',\n",
       " 860: 'kismet',\n",
       " 861: 'pipa',\n",
       " 862: 'desperate',\n",
       " 863: 'misery',\n",
       " 864: 'recovered',\n",
       " 865: 'disappointment',\n",
       " 866: 'goodbye',\n",
       " 867: 'columbia',\n",
       " 868: 'attended',\n",
       " 869: 'engagement',\n",
       " 870: 'cursor',\n",
       " 871: 'dry',\n",
       " 872: 'hoped',\n",
       " 873: 'patents',\n",
       " 874: 'borders',\n",
       " 875: 'terminal',\n",
       " 876: 'subsequently',\n",
       " 877: 'singing',\n",
       " 878: 'attempting',\n",
       " 879: 'visiting',\n",
       " 880: '600',\n",
       " 881: 'recording',\n",
       " 882: 'grasp',\n",
       " 883: '2010',\n",
       " 884: 'thrive',\n",
       " 885: 'evolved',\n",
       " 886: 'lots',\n",
       " 887: 'proves',\n",
       " 888: 'affordability',\n",
       " 889: 'littlebits',\n",
       " 890: 'stewardship',\n",
       " 891: 'stack',\n",
       " 892: 'spheres',\n",
       " 893: 'unpleasant',\n",
       " 894: 'review',\n",
       " 895: 'ethiopia',\n",
       " 896: 'talented',\n",
       " 897: 'crew',\n",
       " 898: \"grandfather's\",\n",
       " 899: 'happened',\n",
       " 900: 'arbitrary',\n",
       " 901: 'maintained',\n",
       " 902: '5',\n",
       " 903: 'supermassive',\n",
       " 904: \"chariot's\",\n",
       " 905: 'anomaly',\n",
       " 906: 'kite',\n",
       " 907: 'theoretical',\n",
       " 908: 'heck',\n",
       " 909: 'vein',\n",
       " 910: 'lasting',\n",
       " 911: 'ecosystem',\n",
       " 912: 'retinal',\n",
       " 913: 'cooked',\n",
       " 914: 'advancing',\n",
       " 915: 'previous',\n",
       " 916: 'slaughter',\n",
       " 917: 'numerical',\n",
       " 918: 'sources',\n",
       " 919: 'surfers',\n",
       " 920: 'involving',\n",
       " 921: 'passive',\n",
       " 922: 'reset',\n",
       " 923: \"person's\",\n",
       " 924: 'expand',\n",
       " 925: 'alberto',\n",
       " 926: 'intuitions',\n",
       " 927: 'illustrates',\n",
       " 928: 'liters',\n",
       " 929: 'manufacture',\n",
       " 930: 'useful',\n",
       " 931: 'approaches',\n",
       " 932: 'superconductor',\n",
       " 933: 'knives',\n",
       " 934: 'competent',\n",
       " 935: 'ministry',\n",
       " 936: 'paulo',\n",
       " 937: 'pittsburgh',\n",
       " 938: 'quit',\n",
       " 939: 'blankets',\n",
       " 940: 'dg',\n",
       " 941: 'presented',\n",
       " 942: 'search',\n",
       " 943: 'entrance',\n",
       " 944: 'relate',\n",
       " 945: 'convenient',\n",
       " 946: 'courtship',\n",
       " 947: 'roof',\n",
       " 948: 'farmers',\n",
       " 949: 'abuse',\n",
       " 950: 'high-speed',\n",
       " 951: 'motor',\n",
       " 952: 'sniff',\n",
       " 953: 'voyager',\n",
       " 954: 'songs',\n",
       " 955: 'somewhere',\n",
       " 956: 'ottoman',\n",
       " 957: 'six-year-old',\n",
       " 958: 'seal',\n",
       " 959: 'carried',\n",
       " 960: \"doesn't\",\n",
       " 961: \"husband's\",\n",
       " 962: 'cups',\n",
       " 963: 'living',\n",
       " 964: 'trap',\n",
       " 965: 'astronomy',\n",
       " 966: 'requirement',\n",
       " 967: 'giving',\n",
       " 968: 'purchases',\n",
       " 969: 'prioritize',\n",
       " 970: 'oil',\n",
       " 971: 'kings',\n",
       " 972: 'breath',\n",
       " 973: 'paradigm',\n",
       " 974: 'come',\n",
       " 975: 'aid',\n",
       " 976: 'contain',\n",
       " 977: 'writing',\n",
       " 978: 'care',\n",
       " 979: 'creepy',\n",
       " 980: 'gamble',\n",
       " 981: 'aged',\n",
       " 982: 'delicate',\n",
       " 983: 'bluetooth',\n",
       " 984: 'dozen',\n",
       " 985: 'diarrheal',\n",
       " 986: 'treatment',\n",
       " 987: 'speaks',\n",
       " 988: 'foremost',\n",
       " 989: 'one-third',\n",
       " 990: 'cult',\n",
       " 991: 'memories',\n",
       " 992: 'variable',\n",
       " 993: 'awarded',\n",
       " 994: '?',\n",
       " 995: 'geometric',\n",
       " 996: 'aquarium',\n",
       " 997: 'parasite',\n",
       " 998: 'democratic',\n",
       " 999: 'photos',\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Model Inputs i.e tensorflow placeholders\n",
    "def get_model_inputs():\n",
    "    '''\n",
    "    Return : input_, target, learning_rate\n",
    "    '''\n",
    "    \n",
    "    input_ = tf.placeholder(dtype=tf.int32,shape=(None,None),name=\"input\")\n",
    "    target = tf.placeholder(dtype=tf.int32,shape=(None,None),name=\"target\")\n",
    "    learning_rate = tf.placeholder(dtype=tf.float32,name=\"learning_rate\")\n",
    "    \n",
    "    return input_, target, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_text_to_int(talks, vocab_to_int):\n",
    "    '''\n",
    "    Params : talks, vocab_to_int\n",
    "    Return : text_to_int\n",
    "    '''\n",
    "    text_to_int = []\n",
    "    for talk in talks:\n",
    "        for token in talk:\n",
    "            if token in vocab_to_int:\n",
    "                text_to_int.append(vocab_to_int[token])\n",
    "    \n",
    "    return np.array(text_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(talks, vocab_to_int, batch_size, seq_length):\n",
    "    '''\n",
    "    Param : source, target\n",
    "    Return : batches\n",
    "    '''\n",
    "    source = convert_text_to_int(talks, vocab_to_int)\n",
    "    batch_count = len(source)//(batch_size * seq_length)\n",
    "    \n",
    "    source = source[:(batch_size * seq_length * batch_count)]\n",
    "    target = np.zeros_like(source)\n",
    "    \n",
    "    target[:-1] = source[1:]\n",
    "    target[-1] = source[0]\n",
    "    \n",
    "    batches = []\n",
    "    source_reshaped = np.reshape(source,(batch_size,-1))\n",
    "    target_reshaped = np.reshape(target,(batch_size,-1))\n",
    "    \n",
    "    for i in range(0,source_reshaped.shape[1],seq_length):\n",
    "        input_ = source_reshaped[:,i:i+seq_length]\n",
    "        target = target_reshaped[:,i:i+seq_length]\n",
    "        batches.append((input_,target))    \n",
    "        \n",
    "    return np.array(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(tokenized_talks, vocab_to_int, 500, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(921, 2, 500, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cell(batch_size,rnn_size,keep_prob,num_layer):\n",
    "    \n",
    "    def get_lstm(rnn_size,keep_prob):\n",
    "        gru = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(cell=gru,input_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_lstm(rnn_size,keep_prob) for _ in range(num_layer)])\n",
    "    \n",
    "    cell_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    initial_state = tf.identity(cell_state,name=\"initial_state\")\n",
    "    print(\"build_cell -> \", initial_state)\n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build Model\n",
    "def build_model(embed, cell, rnn_size, keep_prob=0.5, num_layer=5):\n",
    "    '''\n",
    "    Params : embed, cell, rnn_size, keep_prob=0.5, num_layer=5\n",
    "    Return : output\n",
    "    '''\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=embed, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state,name=\"final_state\")\n",
    "    return outputs, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    '''\n",
    "    Params : input_data, vocab, embed_dim\n",
    "    Return : embed\n",
    "    '''\n",
    "    embedings = tf.Variable(tf.truncated_normal(shape=(vocab_size,embed_dim),stddev=0.1),name=\"embeding\")\n",
    "    embed = tf.nn.embedding_lookup(embedings,input_data)\n",
    "    return embed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_network(cell, input_data, rnn_size, keep_prob, num_layers, vocab_size):\n",
    "    '''\n",
    "    Params : cell, input_data, rnn_size, keep_prob, num_layers, vocab_size)\n",
    "    Return : logits, final_state\n",
    "    '''\n",
    "    embed = get_embed(input_data,vocab_size,embed_dim)\n",
    "    outputs, final_state = build_model(embed,cell,rnn_size,keep_prob,num_layers)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs,vocab_size,activation_fn=None)\n",
    "    return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 20\n",
    "# Batch Size\n",
    "batch_size = 2048\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "#Number of layers\n",
    "num_layers = 2\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 200\n",
    "# Sequence Length\n",
    "seq_length = 20\n",
    "# Learning Rate\n",
    "lr = 0.01\n",
    "#Dropout Prob\n",
    "keep_prob = 0.5\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 10\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_cell ->  Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Build Graph\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    vocab_size = len(int_to_vocab)\n",
    "    \n",
    "    input_, target, learning_rate = get_model_inputs()\n",
    "    \n",
    "    input_data_shape = tf.shape(input_)\n",
    "    \n",
    "    cell, initial_state = build_cell(batch_size=input_data_shape[0],rnn_size=rnn_size, \n",
    "                                     keep_prob=keep_prob, num_layer=num_layers)\n",
    "    \n",
    "    logits, final_state = build_network(cell=cell, input_data=input_,rnn_size=rnn_size,\n",
    "                                        keep_prob=keep_prob,num_layers=num_layers,\n",
    "                                        vocab_size=vocab_size)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits,name=\"probs\")\n",
    "    \n",
    "    # Loss function\n",
    "    masks = tf.ones([input_data_shape[0], input_data_shape[1]])\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(logits,target,masks)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch   0 Batch    0/112   train_loss = 9.511\n",
      "Epoch   0 Batch   10/112   train_loss = 6.528\n",
      "Epoch   0 Batch   20/112   train_loss = 6.413\n",
      "Epoch   0 Batch   30/112   train_loss = 6.373\n",
      "Epoch   0 Batch   40/112   train_loss = 6.336\n",
      "Epoch   0 Batch   50/112   train_loss = 6.333\n",
      "Epoch   0 Batch   60/112   train_loss = 6.322\n",
      "Epoch   0 Batch   70/112   train_loss = 6.316\n",
      "Epoch   0 Batch   80/112   train_loss = 6.280\n",
      "Epoch   0 Batch   90/112   train_loss = 6.229\n",
      "Epoch   0 Batch  100/112   train_loss = 6.178\n",
      "Epoch   0 Batch  110/112   train_loss = 6.158\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch   1 Batch    8/112   train_loss = 6.113\n",
      "Epoch   1 Batch   18/112   train_loss = 6.090\n",
      "Epoch   1 Batch   28/112   train_loss = 6.037\n",
      "Epoch   1 Batch   38/112   train_loss = 6.003\n",
      "Epoch   1 Batch   48/112   train_loss = 5.983\n",
      "Epoch   1 Batch   58/112   train_loss = 5.936\n",
      "Epoch   1 Batch   68/112   train_loss = 5.887\n",
      "Epoch   1 Batch   78/112   train_loss = 5.834\n",
      "Epoch   1 Batch   88/112   train_loss = 5.821\n",
      "Epoch   1 Batch   98/112   train_loss = 5.809\n",
      "Epoch   1 Batch  108/112   train_loss = 5.809\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch   2 Batch    6/112   train_loss = 5.739\n",
      "Epoch   2 Batch   16/112   train_loss = 5.742\n",
      "Epoch   2 Batch   26/112   train_loss = 5.667\n",
      "Epoch   2 Batch   36/112   train_loss = 5.655\n",
      "Epoch   2 Batch   46/112   train_loss = 5.609\n",
      "Epoch   2 Batch   56/112   train_loss = 5.583\n",
      "Epoch   2 Batch   66/112   train_loss = 5.594\n",
      "Epoch   2 Batch   76/112   train_loss = 5.544\n",
      "Epoch   2 Batch   86/112   train_loss = 5.493\n",
      "Epoch   2 Batch   96/112   train_loss = 5.494\n",
      "Epoch   2 Batch  106/112   train_loss = 5.484\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch   3 Batch    4/112   train_loss = 5.480\n",
      "Epoch   3 Batch   14/112   train_loss = 5.456\n",
      "Epoch   3 Batch   24/112   train_loss = 5.450\n",
      "Epoch   3 Batch   34/112   train_loss = 5.437\n",
      "Epoch   3 Batch   44/112   train_loss = 5.437\n",
      "Epoch   3 Batch   54/112   train_loss = 5.414\n",
      "Epoch   3 Batch   64/112   train_loss = 5.406\n",
      "Epoch   3 Batch   74/112   train_loss = 5.373\n",
      "Epoch   3 Batch   84/112   train_loss = 5.374\n",
      "Epoch   3 Batch   94/112   train_loss = 5.385\n",
      "Epoch   3 Batch  104/112   train_loss = 5.352\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch   4 Batch    2/112   train_loss = 5.344\n",
      "Epoch   4 Batch   12/112   train_loss = 5.313\n",
      "Epoch   4 Batch   22/112   train_loss = 5.331\n",
      "Epoch   4 Batch   32/112   train_loss = 5.294\n",
      "Epoch   4 Batch   42/112   train_loss = 5.287\n",
      "Epoch   4 Batch   52/112   train_loss = 5.266\n",
      "Epoch   4 Batch   62/112   train_loss = 5.274\n",
      "Epoch   4 Batch   72/112   train_loss = 5.249\n",
      "Epoch   4 Batch   82/112   train_loss = 5.262\n",
      "Epoch   4 Batch   92/112   train_loss = 5.239\n",
      "Epoch   4 Batch  102/112   train_loss = 5.219\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch   5 Batch    0/112   train_loss = 5.216\n",
      "Epoch   5 Batch   10/112   train_loss = 5.207\n",
      "Epoch   5 Batch   20/112   train_loss = 5.216\n",
      "Epoch   5 Batch   30/112   train_loss = 5.205\n",
      "Epoch   5 Batch   40/112   train_loss = 5.171\n",
      "Epoch   5 Batch   50/112   train_loss = 5.164\n",
      "Epoch   5 Batch   60/112   train_loss = 5.158\n",
      "Epoch   5 Batch   70/112   train_loss = 5.154\n",
      "Epoch   5 Batch   80/112   train_loss = 5.152\n",
      "Epoch   5 Batch   90/112   train_loss = 5.155\n",
      "Epoch   5 Batch  100/112   train_loss = 5.122\n",
      "Epoch   5 Batch  110/112   train_loss = 5.132\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch   6 Batch    8/112   train_loss = 5.106\n",
      "Epoch   6 Batch   18/112   train_loss = 5.113\n",
      "Epoch   6 Batch   28/112   train_loss = 5.107\n",
      "Epoch   6 Batch   38/112   train_loss = 5.095\n",
      "Epoch   6 Batch   48/112   train_loss = 5.092\n",
      "Epoch   6 Batch   58/112   train_loss = 5.091\n",
      "Epoch   6 Batch   68/112   train_loss = 5.076\n",
      "Epoch   6 Batch   78/112   train_loss = 5.048\n",
      "Epoch   6 Batch   88/112   train_loss = 5.048\n",
      "Epoch   6 Batch   98/112   train_loss = 5.065\n",
      "Epoch   6 Batch  108/112   train_loss = 5.072\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch   7 Batch    6/112   train_loss = 5.017\n",
      "Epoch   7 Batch   16/112   train_loss = 5.025\n",
      "Epoch   7 Batch   26/112   train_loss = 5.002\n",
      "Epoch   7 Batch   36/112   train_loss = 5.018\n",
      "Epoch   7 Batch   46/112   train_loss = 5.000\n",
      "Epoch   7 Batch   56/112   train_loss = 4.990\n",
      "Epoch   7 Batch   66/112   train_loss = 5.016\n",
      "Epoch   7 Batch   76/112   train_loss = 4.981\n",
      "Epoch   7 Batch   86/112   train_loss = 4.946\n",
      "Epoch   7 Batch   96/112   train_loss = 4.960\n",
      "Epoch   7 Batch  106/112   train_loss = 4.960\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch   8 Batch    4/112   train_loss = 4.954\n",
      "Epoch   8 Batch   14/112   train_loss = 4.942\n",
      "Epoch   8 Batch   24/112   train_loss = 4.944\n",
      "Epoch   8 Batch   34/112   train_loss = 4.934\n",
      "Epoch   8 Batch   44/112   train_loss = 4.956\n",
      "Epoch   8 Batch   54/112   train_loss = 4.935\n",
      "Epoch   8 Batch   64/112   train_loss = 4.919\n",
      "Epoch   8 Batch   74/112   train_loss = 4.883\n",
      "Epoch   8 Batch   84/112   train_loss = 4.898\n",
      "Epoch   8 Batch   94/112   train_loss = 4.906\n",
      "Epoch   8 Batch  104/112   train_loss = 4.893\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch   9 Batch    2/112   train_loss = 4.891\n",
      "Epoch   9 Batch   12/112   train_loss = 4.865\n",
      "Epoch   9 Batch   22/112   train_loss = 4.880\n",
      "Epoch   9 Batch   32/112   train_loss = 4.859\n",
      "Epoch   9 Batch   42/112   train_loss = 4.854\n",
      "Epoch   9 Batch   52/112   train_loss = 4.835\n",
      "Epoch   9 Batch   62/112   train_loss = 4.856\n",
      "Epoch   9 Batch   72/112   train_loss = 4.847\n",
      "Epoch   9 Batch   82/112   train_loss = 4.855\n",
      "Epoch   9 Batch   92/112   train_loss = 4.836\n",
      "Epoch   9 Batch  102/112   train_loss = 4.834\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch  10 Batch    0/112   train_loss = 4.822\n",
      "Epoch  10 Batch   10/112   train_loss = 4.809\n",
      "Epoch  10 Batch   20/112   train_loss = 4.826\n",
      "Epoch  10 Batch   30/112   train_loss = 4.833\n",
      "Epoch  10 Batch   40/112   train_loss = 4.799\n",
      "Epoch  10 Batch   50/112   train_loss = 4.795\n",
      "Epoch  10 Batch   60/112   train_loss = 4.803\n",
      "Epoch  10 Batch   70/112   train_loss = 4.799\n",
      "Epoch  10 Batch   80/112   train_loss = 4.803\n",
      "Epoch  10 Batch   90/112   train_loss = 4.798\n",
      "Epoch  10 Batch  100/112   train_loss = 4.780\n",
      "Epoch  10 Batch  110/112   train_loss = 4.794\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch  11 Batch    8/112   train_loss = 4.765\n",
      "Epoch  11 Batch   18/112   train_loss = 4.782\n",
      "Epoch  11 Batch   28/112   train_loss = 4.783\n",
      "Epoch  11 Batch   38/112   train_loss = 4.772\n",
      "Epoch  11 Batch   48/112   train_loss = 4.766\n",
      "Epoch  11 Batch   58/112   train_loss = 4.778\n",
      "Epoch  11 Batch   68/112   train_loss = 4.762\n",
      "Epoch  11 Batch   78/112   train_loss = 4.739\n",
      "Epoch  11 Batch   88/112   train_loss = 4.744\n",
      "Epoch  11 Batch   98/112   train_loss = 4.769\n",
      "Epoch  11 Batch  108/112   train_loss = 4.783\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch  12 Batch    6/112   train_loss = 4.727\n",
      "Epoch  12 Batch   16/112   train_loss = 4.738\n",
      "Epoch  12 Batch   26/112   train_loss = 4.721\n",
      "Epoch  12 Batch   36/112   train_loss = 4.750\n",
      "Epoch  12 Batch   46/112   train_loss = 4.731\n",
      "Epoch  12 Batch   56/112   train_loss = 4.721\n",
      "Epoch  12 Batch   66/112   train_loss = 4.751\n",
      "Epoch  12 Batch   76/112   train_loss = 4.719\n",
      "Epoch  12 Batch   86/112   train_loss = 4.692\n",
      "Epoch  12 Batch   96/112   train_loss = 4.711\n",
      "Epoch  12 Batch  106/112   train_loss = 4.713\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch  13 Batch    4/112   train_loss = 4.716\n",
      "Epoch  13 Batch   14/112   train_loss = 4.694\n",
      "Epoch  13 Batch   24/112   train_loss = 4.698\n",
      "Epoch  13 Batch   34/112   train_loss = 4.698\n",
      "Epoch  13 Batch   44/112   train_loss = 4.727\n",
      "Epoch  13 Batch   54/112   train_loss = 4.707\n",
      "Epoch  13 Batch   64/112   train_loss = 4.690\n",
      "Epoch  13 Batch   74/112   train_loss = 4.666\n",
      "Epoch  13 Batch   84/112   train_loss = 4.674\n",
      "Epoch  13 Batch   94/112   train_loss = 4.696\n",
      "Epoch  13 Batch  104/112   train_loss = 4.686\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch  14 Batch    2/112   train_loss = 4.686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  14 Batch   12/112   train_loss = 4.671\n",
      "Epoch  14 Batch   22/112   train_loss = 4.677\n",
      "Epoch  14 Batch   32/112   train_loss = 4.656\n",
      "Epoch  14 Batch   42/112   train_loss = 4.655\n",
      "Epoch  14 Batch   52/112   train_loss = 4.640\n",
      "Epoch  14 Batch   62/112   train_loss = 4.668\n",
      "Epoch  14 Batch   72/112   train_loss = 4.659\n",
      "Epoch  14 Batch   82/112   train_loss = 4.672\n",
      "Epoch  14 Batch   92/112   train_loss = 4.655\n",
      "Epoch  14 Batch  102/112   train_loss = 4.654\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch  15 Batch    0/112   train_loss = 4.642\n",
      "Epoch  15 Batch   10/112   train_loss = 4.631\n",
      "Epoch  15 Batch   20/112   train_loss = 4.654\n",
      "Epoch  15 Batch   30/112   train_loss = 4.666\n",
      "Epoch  15 Batch   40/112   train_loss = 4.637\n",
      "Epoch  15 Batch   50/112   train_loss = 4.634\n",
      "Epoch  15 Batch   60/112   train_loss = 4.643\n",
      "Epoch  15 Batch   70/112   train_loss = 4.641\n",
      "Epoch  15 Batch   80/112   train_loss = 4.643\n",
      "Epoch  15 Batch   90/112   train_loss = 4.635\n",
      "Epoch  15 Batch  100/112   train_loss = 4.624\n",
      "Epoch  15 Batch  110/112   train_loss = 4.638\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch  16 Batch    8/112   train_loss = 4.612\n",
      "Epoch  16 Batch   18/112   train_loss = 4.628\n",
      "Epoch  16 Batch   28/112   train_loss = 4.634\n",
      "Epoch  16 Batch   38/112   train_loss = 4.622\n",
      "Epoch  16 Batch   48/112   train_loss = 4.625\n",
      "Epoch  16 Batch   58/112   train_loss = 4.636\n",
      "Epoch  16 Batch   68/112   train_loss = 4.626\n",
      "Epoch  16 Batch   78/112   train_loss = 4.602\n",
      "Epoch  16 Batch   88/112   train_loss = 4.607\n",
      "Epoch  16 Batch   98/112   train_loss = 4.636\n",
      "Epoch  16 Batch  108/112   train_loss = 4.651\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch  17 Batch    6/112   train_loss = 4.592\n",
      "Epoch  17 Batch   16/112   train_loss = 4.615\n",
      "Epoch  17 Batch   26/112   train_loss = 4.596\n",
      "Epoch  17 Batch   36/112   train_loss = 4.616\n",
      "Epoch  17 Batch   46/112   train_loss = 4.606\n",
      "Epoch  17 Batch   56/112   train_loss = 4.604\n",
      "Epoch  17 Batch   66/112   train_loss = 4.627\n",
      "Epoch  17 Batch   76/112   train_loss = 4.600\n",
      "Epoch  17 Batch   86/112   train_loss = 4.574\n",
      "Epoch  17 Batch   96/112   train_loss = 4.586\n",
      "Epoch  17 Batch  106/112   train_loss = 4.597\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch  18 Batch    4/112   train_loss = 4.599\n",
      "Epoch  18 Batch   14/112   train_loss = 4.585\n",
      "Epoch  18 Batch   24/112   train_loss = 4.597\n",
      "Epoch  18 Batch   34/112   train_loss = 4.596\n",
      "Epoch  18 Batch   44/112   train_loss = 4.617\n",
      "Epoch  18 Batch   54/112   train_loss = 4.595\n",
      "Epoch  18 Batch   64/112   train_loss = 4.592\n",
      "Epoch  18 Batch   74/112   train_loss = 4.561\n",
      "Epoch  18 Batch   84/112   train_loss = 4.573\n",
      "Epoch  18 Batch   94/112   train_loss = 4.587\n",
      "Epoch  18 Batch  104/112   train_loss = 4.581\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 256), dtype=float32)\n",
      "Epoch  19 Batch    2/112   train_loss = 4.586\n",
      "Epoch  19 Batch   12/112   train_loss = 4.571\n",
      "Epoch  19 Batch   22/112   train_loss = 4.577\n",
      "Epoch  19 Batch   32/112   train_loss = 4.557\n",
      "Epoch  19 Batch   42/112   train_loss = 4.558\n",
      "Epoch  19 Batch   52/112   train_loss = 4.539\n",
      "Epoch  19 Batch   62/112   train_loss = 4.572\n",
      "Epoch  19 Batch   72/112   train_loss = 4.568\n",
      "Epoch  19 Batch   82/112   train_loss = 4.577\n",
      "Epoch  19 Batch   92/112   train_loss = 4.563\n",
      "Epoch  19 Batch  102/112   train_loss = 4.553\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "#Training Network\n",
    "\n",
    "batches = get_batches(tokenized_talks, vocab_to_int, batch_size, seq_length)\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_i in range(num_epochs):\n",
    "        print(initial_state)\n",
    "        state = sess.run(initial_state,feed_dict={input_:batches[0][0]})\n",
    "        \n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_: x,\n",
    "                target: y,\n",
    "                initial_state:state,\n",
    "                learning_rate: lr}\n",
    "            train_loss, state, _ = sess.run([cost,final_state,train_op], feed)\n",
    "            \n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                    print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                        epoch_i,\n",
    "                        batch_i,\n",
    "                        len(batches),\n",
    "                        train_loss))\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    probabilities = np.argsort(probabilities,axis=None)[-10:]\n",
    "#     print(probabilities)\n",
    "    return int_to_vocab[random.choice(probabilities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    inputs = loaded_graph.get_tensor_by_name('input:0')\n",
    "    init_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    return inputs, init_state, final_state, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "in , and this was the story in which it would actually get this on and on the streets and make them a little way on the left place . and it can be the , so that i would say that the word could become , and then that i thought we could make them to get to our and more . now there's not some point of the future we need to have an . we can get this , that we can do this for all kinds about . i can look back with one , you need a look for you from this in this country to this point . you need , but the way you're not really interested from \" \" what ? the world was . the only one that they would go , but it's very interesting at their work or even the best way ? how much are people doing , so they can do their work and not ? how they would . and then what happens , the way i think is to do this because it would take the future of it ? what do we have this kind ? we know the future has been a little sticker , we know about . but i was really excited that i'm talking to you was what i did with that experience to work , and we could never see a bunch about and the way to do with a very special experience , to make these two , a way for a long . but that's a , we need , and so the next thing we need on with you to the city in that , and so they were able in our . but in particular of course people would come into , like a very small village and i thought it didn't want it in . i mean we had been working at some end to make the power for the entire war . and i think it wasn't the case to . the idea , the other is to the public and not in your own job if i had this kind that we should get out in our hands that way to do that before you can have been going for me on that , i can see this kind . i'm going to have to take some of that , so that you have no way and get some point of my parents at this point that i could call them on this stage . you get into an end of these , i was at that point and my life and that i can say . ( music , i'm going about to a very different kind in new york ? well you are going back with the . ( audience cheering later my god ? i have this big quote from me . i'm going home in some kind in this\n"
     ]
    }
   ],
   "source": [
    "word = \"in\"\n",
    "# In the next\n",
    "gen_length = 500\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(\"save.meta\")\n",
    "    loader.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "    \n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [word]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "    \n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "        \n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
    "        gen_sentences.append(pred_word)\n",
    "        \n",
    "    final_talk = \" \".join(gen_sentences)\n",
    "    \n",
    "    print(final_talk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
